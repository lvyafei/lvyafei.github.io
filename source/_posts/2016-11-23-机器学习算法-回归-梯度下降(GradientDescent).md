---
layout: lay_post
title: "机器学习算法-回归-梯度下降(GradientDescent)"
date: 2016-11-23
categories: 回归算法
tags: 机器学习
author: lvyafei
---

## 0.概述

回归分析（regression analysis)是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。运用十分广泛，回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；在线性回归中，按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。如果在回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且自变量之间存在线性相关，则称为多元线性回归分析。

在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。线性回归的主要问题是参数估计。
<!-- more -->

## 1.单参数-LinearRegression(线性回归)

**Hypothesis Function(假设函数):**

![假设函数](/images/算法/逻辑回归/单参数-线性-假设函数.png)

**Cost Function(代价函数):**

![代价函数](/images/算法/逻辑回归/单参数-线性-代价函数.png)

**Gradient Descent(梯度下降算法):**

![梯度下降](/images/算法/逻辑回归/单参数-线性-梯度下降.png)

重复执行，直至收敛

**Gradient Descent for Linear Regression(针对线性回归的梯度下降算法):**

![梯度下降1](/images/算法/逻辑回归/单参数-线性-线性梯度下降.png)

## 2.多参数-LinearRegression(线性回归)

**Hypothesis Function(假设函数):**

![假设函数](/images/算法/逻辑回归/多参数-线性-假设函数.png)

向量化表示:

![假设函数向量化](/images/算法/逻辑回归/多参数-线性-假设函数-向量化.png)

多训练集下：

![多训练集](/images/算法/逻辑回归/多参数-线性-假设函数-向量化-多训练.png)

对应的假设函数:

![假设函数](/images/算法/逻辑回归/多参数-线性-假设函数-向量化-多训练-假设函数.png)

**Cost Function(代价函数):**

![代价函数](/images/算法/逻辑回归/多参数-线性-代价函数.png)

**Gradient Descent for Multiple Linear Regression(针对多参数线性回归的梯度下降算法):**

![梯度下降](/images/算法/逻辑回归/多参数-线性-梯度下降.png)

**Normal Equation(正规方程):**

正规方程是从最小二乘法(矩阵形式)推导过来的，其本质是对代价函数求导，让导数等于0即可求极值，得到最优参数。

![正规方程](/images/算法/逻辑回归/多参数-线性-正规方程.png)

## 3.参数估计算法概述

参数估计是指在已知系统模型结构时，用系统的输入和输出数据计算系统模型参数的过程。18世纪末德国数学家C.F.高斯首先提出参数估计的方法，他用最小二乘法计算天体运行的轨道。20世纪60年代，随着电子计算机的普及，参数估计有了飞速的发展。参数估计有多种方法，有矩估计、极大似然法、一致最小方差无偏估计、最小风险估计、同变估计、最小二乘法、贝叶斯估计、极大验后法、最小风险法和极小化极大熵法等。最基本的方法是最小二乘法和极大似然法。

最小二乘法的目标：求误差的最小平方和，对应有两种：线性和非线性。线性最小二乘的解是closed-form即，而非线性最小二乘没有closed-form，通常用迭代法求解。

迭代法，即在每一步update未知量逐渐逼近解，可以用于各种各样的问题（包括最小二乘），比如求的不是误差的最小平方和而是最小立方和。梯度下降是迭代法的一种，可以用于求解最小二乘问题（线性和非线性都可以）。高斯-牛顿法是另一种经常用于求解非线性最小二乘的迭代法（一定程度上可视为标准非线性最小二乘求解方法）。

还有一种叫做Levenberg-Marquardt的迭代法用于求解非线性最小二乘问题，就结合了梯度下降和高斯-牛顿法。